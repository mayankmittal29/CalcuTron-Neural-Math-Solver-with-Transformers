# üßÆ CalcuTron ü§ñ - "Neural Arithmetic with Transformers: Learning to Calculate Like a Machine"


![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![License](https://img.shields.io/badge/license-MIT-green)
[![Made with ‚ù§Ô∏è](https://img.shields.io/badge/Made%20with-‚ù§Ô∏è-red)](https://github.com/yourusername/CalcuTron)

**CalcuTron** is a cutting-edge üöÄ transformer-based model for solving arithmetic operations with neural networks. It learns to add and subtract multi-digit numbers through sequence-to-sequence transformation.

## üîç Project Objective

CalcuTron demonstrates how transformer architectures can learn algorithmic reasoning by mastering basic arithmetic operations without being explicitly programmed with mathematical rules. The project shows how attention mechanisms can:

- ‚úÖ Learn to carry digits in addition
- ‚úÖ Handle borrowing in subtraction
- ‚úÖ Generalize to longer digit sequences
- ‚úÖ Process multiple operations with a single model

## üß† Model Architecture

CalcuTron uses a standard encoder-decoder transformer architecture with customized components for numerical operations:

```python
class Transformer(nn.Module):
    def __init__(self, vocab_size, N=3, d_model=128, d_ff=512, h=8, dropout=0.1):
        super(Transformer, self).__init__()
        self.encoder = Encoder(EncoderLayer(d_model, MultiHeadAttention(h, d_model), 
                                          PositionwiseFeedForward(d_model, d_ff), dropout), N)
        self.decoder = Decoder(DecoderLayer(d_model, MultiHeadAttention(h, d_model),
                                          MultiHeadAttention(h, d_model),
                                          PositionwiseFeedForward(d_model, d_ff), dropout), N)
        self.src_embed = nn.Sequential(Embeddings(d_model, vocab_size), 
                                     PositionalEncoding(d_model, dropout))
        self.tgt_embed = nn.Sequential(Embeddings(d_model, vocab_size), 
                                     PositionalEncoding(d_model, dropout))
        self.generator = Generator(d_model, vocab_size)
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)
```

### üîë Key Components

- **Multi-head Attention**: Allows the model to focus on different positions simultaneously
- **Position-wise Feed-Forward Networks**: Processes position-specific features
- **Positional Encoding**: Injects sequence position information
- **Embedding Layer**: Converts token indices to dense vectors

## üìÅ Directory Structure

```
CalcuTron/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # Makes src a package
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # Makes data a package
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py        # Dataset and data loader implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py          # Data utilities
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # Makes model a package
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attention.py      # Attention mechanisms
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py     # Token and positional embeddings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encoder.py        # Transformer encoder implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ decoder.py        # Transformer decoder implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layers.py         # Transformer layers and components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformer.py    # Main transformer model
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # Makes utils a package
‚îÇ       ‚îî‚îÄ‚îÄ helpers.py        # Helper functions for training and inference
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train.py              # Training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py           # Evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ analyze.py            # Analysis script
‚îÇ   ‚îî‚îÄ‚îÄ inference.py          # Inference script for making predictions
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ requirements.txt          # Project dependencies
‚îî‚îÄ‚îÄ LICENSE                   # MIT License
```

## üöÄ Getting Started

### Prerequisites

```bash
pip install -r requirements.txt
```

### Training the Model

```bash
python scripts/train.py --train_size 50000 --val_size 5000 --max_digits 3 --operations "+" "-"
```

### Evaluation

```bash
python scripts/evaluate.py --model_path best_arithmetic_transformer.pt --test_size 5000
```

### Inference

```bash
python scripts/inference.py --expression "123+456"
```

## üìä Results

CalcuTron achieves impressive results on arithmetic operations:

| Dataset | Exact Match Accuracy | Character-Level Accuracy | Perplexity |
|---------|----------------------|--------------------------|------------|
| Test    | 99.80%                | 99.89%                    | 1.0028       |
| Generalization | 0.0%         | 15.33%                    | 7202.9410       |

### ‚ú® Performance by Operation Type

![Performance by Operation](https://via.placeholder.com/650x400?text=Performance+by+Operation)

- Addition: 99.7% accuracy
- Subtraction: 99.6% accuracy

### üìà Performance by Input Length

![Performance by Length](https://via.placeholder.com/650x400?text=Performance+by+Length)

The model maintains >99% accuracy for expressions up to 6 digits in length, showing strong generalization capabilities.

## üî¨ Ablation Studies

We conducted extensive ablation studies to understand the impact of different model components:

```python
ablation_configs = {
    'baseline': {'N': 3, 'd_model': 128, 'd_ff': 512, 'h': 8, 'dropout': 0.1},
    'fewer_layers': {'N': 2, 'd_model': 128, 'd_ff': 512, 'h': 8, 'dropout': 0.1},
    'smaller_model_dim': {'N': 3, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
}
```

Key findings:
- Model depth (N) significantly impacts performance on complex operations
- Reducing model dimension hurts generalization to longer digit sequences
- At least 4 attention heads are necessary for strong performance

## üõ†Ô∏è Future Work

- [ ] Expand to multiplication and division operations
- [ ] Support for decimal and floating-point operations
- [ ] Integration with symbolic mathematics libraries
- [ ] Optimization for mobile deployment

## üôè Acknowledgments

- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) for the transformer implementation inspiration
- [PyTorch team](https://pytorch.org/) for the excellent deep learning framework

---

<div align="center">
  <strong>Made with ‚ù§Ô∏è by <a href="https://github.com/yourusername">Your Name</a></strong>
</div>
